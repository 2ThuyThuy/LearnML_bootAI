{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c164fbb",
   "metadata": {},
   "source": [
    "# 1. Evaluating Regression Models\n",
    "## 1.1. R Square/Adjusted R Square\n",
    "\n",
    "R Square is the square of the Correlation Coefficient(R). It calculated by the sum of squared of prediction error divided by the total sum of the square which replaces the calculated prediction with mean.\n",
    "\n",
    "$$ R ^ 2 =  1 - \\frac{\\text{SS}_\\text{Regression}}{\\text{SS}_\\text{total}} = 1 - \\frac{\\sum_i(y_i- \\hat{y}_i)^2}{\\sum_i(y_i- \\bar{y}_i)^2}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e195dc28",
   "metadata": {},
   "source": [
    "R Square is good measure to determine, however, it does not take consideration of overfiting problem.\n",
    "\n",
    "**R Square is better used to explain the model to other people because you can explain the number as a percentage of the output variability**\n",
    "\n",
    "=> The higher the $R^2$ values, the better a model fits a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac83eda",
   "metadata": {},
   "source": [
    "## 1.2 Mean Square Error(MSE)/ Root Mean Square Error(RMSE)\n",
    "Mean Square Error is an absolute meansure of the goodness for the fit. It calculated by the sum of square of prediction error which is real output minus predicted output and then divide by the number of data points. \n",
    "\n",
    "$$MSE = \\frac{1}{N} \\sum^N_{i=1}(y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "- $y_i$ is y_predict\n",
    "- $\\hat{y}_i$ is y_test\n",
    "\n",
    "Root Mean Square Error (RMSE) is square root of MSE. It is used more commonly than MSE because firstly sometime MSE value can be too big to compare easily. Secondly, MSE is calcuated by the square of error, and thus square root brings it back to the same level of prediction error and makes it easier for interpretation.\n",
    "\n",
    "=> The lower the RMSE the better a model fits a dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f12c75c",
   "metadata": {},
   "source": [
    "## 1.3 Mean Absolute Error (MAE)\n",
    "Mean Absolute Error(MAE) calcatuted by the sum of the absolute value of error.\n",
    "\n",
    "$$MSE = \\frac{1}{N} \\sum^N_{i=1} |y_i - \\hat{y}_i|$$\n",
    "\n",
    "MSE give larger penalization to big prediction error by square it while MAE treats all errors the same"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a97c82",
   "metadata": {},
   "source": [
    "**MSE, RMSE or MAE are better be used to compare performance between different regression models.** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5087f74a",
   "metadata": {},
   "source": [
    "# 2. Decission Tree Regression \n",
    "Decission tree builds regression or classification models in the form of a tree structure. It breaks down a dataset into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed. The final result is a tree with **decission nodes** and **leaf nodes**.\n",
    "\n",
    "Tree algorithms: ID3, C4.5, C5.0 and CART :\n",
    "\n",
    "- ID3 (Iterative Dichotomiser 3) creates a multiway tree, finding for each node (gready manner) the categorical feature that will yield the largest information gain for categorical targets. Trees are grown to their maximum size and then a pruning step is usually applied to improve the ability of the tree to generalize to unseen data.\n",
    "\n",
    "- C4.5 is the successor to ID3 and removed the restriction that features must be categorycal by dynamically defining a discrete attribute (based on numerical variables) that partitions the continuous attribute value into a discrete set of intervals. C4.5 converts the trained tree (i.e. the output of the ID3 algorithm) into sets of if-then rules. These accuracy of each rule is then evaluated to determine the order in which they should be applied. Pruning is done by removing a rule's precondition if the accuracy of the rule improves without it.\n",
    "\n",
    "- CART (Classification and Regression Trees) is very similar to C4.5. But is differs in that it supports numerical target variables (regression) and does not compute rule sets. CART constructs binary trees using the features and threshold that yield the largest information gain each node. \n",
    "\n",
    "    => sklearn use CART algorithms, however scikit-learn doesn't support categorical variables for now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ef7d49",
   "metadata": {},
   "source": [
    "# 3. Ensemble methods\n",
    "\n",
    "The goal of ensemble methods is combine the predictions of several base estimators built with a given learning algorithm in order to improve generalizability/ robustness over a single estimator.\n",
    "\n",
    "- **averaging methods**\n",
    "- **boosting methods**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb02d18",
   "metadata": {},
   "source": [
    "## 3.1 Random Forest Regressor\n",
    "Each tree in the ensemble is built from a sample drawn with replacement from the training set.\n",
    "\n",
    "When spliting each node during the construction of a tree, the best split is found either from all input features or a random subset of size max_features.\n",
    "\n",
    "The purpose of these two sources of randomness is to decrease the variance of the forest esimator. Indeed, individual decision trees typically exhibit high variance and tend to overfit. The injected randomness in forests yield decision trees with somewhat decoupled prediction errors. By taking an average of those predictions, some errors can cancel out. Random forests achieve a reduced variance by combining diverse trees, sometimes at the cose of a slight increase in bias. Inpractice the variance reduction is often significatn hence yielding an overall better model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293646b0",
   "metadata": {},
   "source": [
    "## 3.2  AdaBoost \n",
    "The core principle of AdaBoost is to fit a sequence of weak learners on repeatedly modified versions of the data. The predictions from all of them are then combined through a weighted majority vote to producre the final prediction. The data modification at each so-called boosting iteration consist of applying weights $w_1, w2_, \\dots, w_n$ to to each of the training samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724ebdf5",
   "metadata": {},
   "source": [
    "## 3.3 Gradient Tree Boosting\n",
    "Gradient Tree boosting of Gradient Boosted Decision Trees (GBDT) is a generalization of boosting to arbitrary differentiable loss functions. GBDT is an accurate and effective off-the shelf procedure that can be used for both regression and classification problem in a variety of areas inclusing Web search ranking and ecology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9abc3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
